{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myGdUAyg3so2"
      },
      "source": [
        "# 2110446 Data Science and Data Engineering\n",
        "# Preparing and Cleaning Data for Machine Learning\n",
        "\n",
        "Credit: https://www.dataquest.io/blog/machine-learning-preparing-data/\n",
        "\n",
        "![](https://github.com/kaopanboonyuen/2110446_DataScience_2021s2/raw/main/code/week2_dataprep/loanpre-thumbnail-credit.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTakpG_E3so5"
      },
      "source": [
        "First, lets import some of the libraries that we'll be using, and set some parameters to make the output easier to read."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faUp_jyV3so6"
      },
      "source": [
        "# 1) Examining the Data Set\n",
        "Lending Club periodically releases data for all the approved and declined loan applications on their website. So you're working with the same data we are, we've mirrored the data on data.world. You can select different year ranges to download the dataset (in CSV format) for both approved and declined loans.\n",
        "\n",
        "You'll also find a data dictionary (in XLS format), towards the bottom of the page, which contains information on the different column names. The data dictionary is useful to help understand what a column represents in the dataset.\n",
        "\n",
        "The data dictionary contains two sheets:\n",
        "\n",
        "LoanStats sheet: describes the approved loans dataset\n",
        "RejectStats sheet: describes the rejected loans dataset\n",
        "We'll be using the LoanStats sheet since we're interested in the approved loans dataset.\n",
        "\n",
        "The approved loans dataset contains information on current loans, completed loans, and defaulted loans. For this challenge, we'll be working with approved loans data for the years 2007 to 2011.\n",
        "\n",
        "First, lets import some of the libraries that we'll be using, and set some parameters to make the output easier to read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxMizIzE3so7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# set this option to wrap wide columns\n",
        "pd.set_option('max_columns', 120)\n",
        "pd.set_option('max_colwidth', 5000)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12,8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgyI3C7B3so_"
      },
      "source": [
        "## Loading The Data Into Pandas\n",
        "\n",
        "We've downloaded our dataset and named it lending_club_loans.csv, but now we need to load it into a pandas DataFrame to explore it.\n",
        "\n",
        "To ensure that code run fast for us, we need to reduce the size of lending_club_loans.csv by doing the following:\n",
        "\n",
        "Remove the first line: It contains extraneous text instead of the column titles. This text prevents the dataset from being parsed properly by the pandas library.\n",
        "Remove the 'desc' column: it contains a long text explanation for the loan.\n",
        "Remove the 'url' column: it contains a link to each on Lending Club which can only be accessed with an investor account.\n",
        "Removing all columns with more than 50% missing values: This allows us to move faster since don't need to spend time trying to fill these values.\n",
        "We'll also name the filtered dataset loans_2007 and later at the end of this section save it as loans_2007.csv to keep it separate from the raw data. This is good practice and makes sure we have our original data in case we need to go back and retrieve any of the original data we're removing.\n",
        "\n",
        "Now, let's go ahead and perform these steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFW46PXZ3spA"
      },
      "outputs": [],
      "source": [
        "# skip row 1 so pandas can parse the data properly.\n",
        "loans_2007 = pd.read_csv('https://github.com/kaopanboonyuen/2110446_DataScience_2021s2/raw/main/datasets/lending_club_loans.csv', low_memory=False) \n",
        "print (loans_2007.shape)\n",
        "half_count = len(loans_2007) / 2\n",
        "print(half_count)\n",
        "\n",
        "loans_2007 = loans_2007.dropna(thresh=half_count,axis=1) # Drop any column with more than 50% missing values\n",
        "print (loans_2007.shape)\n",
        "\n",
        "loans_2007 = loans_2007.drop(['url','desc'],axis=1)      # These columns are not useful for our purposes\n",
        "print (loans_2007.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu6njEE83spF"
      },
      "source": [
        "## Let's use the pandas head() method \n",
        "to display first three rows of the loans_2007 DataFrame, just to make sure we were able to load the dataset properly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCGFxOdC3spG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loans_2007.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGC4Vc_-3spJ"
      },
      "outputs": [],
      "source": [
        "loans_2007.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzJ4K3lI3spN"
      },
      "source": [
        "## Let's also use pandas .shape attribute \n",
        "to view the number of samples and features we're dealing with at this stage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFUBVa8X3spO"
      },
      "outputs": [],
      "source": [
        "loans_2007.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8bujock3spS"
      },
      "source": [
        "# 2) Narrowing down our columns\n",
        "It's a great idea to spend some time to familiarize ourselves with the columns in the dataset, to understand what each feature represents. This is important, because a poor understanding of the features could cause us to make mistakes in the data analysis and the modeling process.\n",
        "\n",
        "We'll be using the data dictionary Lending Club provided to help us become familiar with the columns and what each represents in the dataset. To make the process easier, we'll create a DataFrame to contain the names of the columns, data type, first row's values, and description from the data dictionary.\n",
        "\n",
        "To make this easier, we've pre-converted the data dictionary from Excel format to a CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5WZqOJL3spT"
      },
      "outputs": [],
      "source": [
        "data_dictionary = pd.read_csv('https://github.com/kaopanboonyuen/Python-Data-Science/raw/master/DataPreparation_toStudent/Lab1_LoansDataSet/dataset/LCDataDictionary.csv') # Loading in the data dictionary\n",
        "print(data_dictionary.shape[0])\n",
        "print(data_dictionary.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIiu7nvY3spW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_dictionary.head()\n",
        "data_dictionary = data_dictionary.rename(columns={'LoanStatNew': 'name','Description': 'description'})\n",
        "print(data_dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmjmFhHa3spa"
      },
      "source": [
        "## Now that we've got the data dictionary loaded.\n",
        "Let's join the first row of loans_2007 to the data_dictionary DataFrame to give us a preview DataFrame with the following columns:\n",
        "\n",
        "name — contains the column names of loans_2007.\n",
        "dtypes — contains the data types of the loans_2007 columns.\n",
        "first value — contains the values of loans_2007 first row.\n",
        "description — explains what each column in loans_2007 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOUyRYaS3spb",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "loans_2007_dtypes = pd.DataFrame(loans_2007.dtypes,columns=['dtypes'])\n",
        "loans_2007_dtypes = loans_2007_dtypes.reset_index()\n",
        "loans_2007_dtypes['name'] = loans_2007_dtypes['index'] # rename column\n",
        "# display(loans_2007_dtypes.head())\n",
        "loans_2007_dtypes = loans_2007_dtypes[['name','dtypes']] # select 2 columns\n",
        "\n",
        "# create column 'first value' to show 1st row of data\n",
        "loans_2007_dtypes['first value'] = loans_2007.loc[0].values \n",
        "\n",
        "# create column 'description' by joining to data_dictionary\n",
        "preview = loans_2007_dtypes.merge(data_dictionary, on='name', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJd-aERr3spe"
      },
      "outputs": [],
      "source": [
        "preview.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxPq6qBD3sph"
      },
      "source": [
        "When we printed the shape of loans_2007 earlier, we noticed that it had 56 columns which also means this preview DataFrame has 56 rows. It can be cumbersome to try to explore all the rows of preview at once, so instead we'll break it up into three parts and look at smaller selection of features each time.\n",
        "\n",
        "As you explore the features to better understand each of them, you'll want to pay attention to any column that:\n",
        "\n",
        "leaks information from the future (after the loan has already been funded),\n",
        "don't affect the borrower's ability to pay back the loan (e.g. a randomly generated ID value by Lending Club),\n",
        "is formatted poorly,\n",
        "requires more data or a lot of preprocessing to turn into useful a feature, or\n",
        "contains redundant information.\n",
        "I'll say it again to emphasize it because it's important: We need to especially pay close attention to data leakage, which can cause the model to overfit. This is because the model would be also learning from features that wouldn't be available when we're using it make predictions on future loans.\n",
        "\n",
        "First Group Of Columns\n",
        "Let's display the first 19 rows of preview and analyze them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya8yxlpo3spj"
      },
      "outputs": [],
      "source": [
        "preview[:19] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyEiWsic3spm"
      },
      "source": [
        "After analyzing the columns, we can conclude that the following features can be removed:\n",
        "\n",
        "id — randomly generated field by Lending Club for unique identification purposes only.\n",
        "member_id — also randomly generated field by Lending Club for identification purposes only.\n",
        "funded_amnt — leaks information from the future(after the loan is already started to be funded).\n",
        "funded_amnt_inv — also leaks data from the future.\n",
        "sub_grade — contains redundant information that is already in the grade column (more below).\n",
        "int_rate — also included within the grade column.\n",
        "emp_title — requires other data and a lot of processing to become potentially useful\n",
        "issued_d — leaks data from the future.\n",
        "Lending Club uses a borrower's grade and payment term (30 or months) to assign an interest rate (you can read more about Rates & Fees). This causes variations in interest rate within a given grade. But, what may be useful for our model is to focus on clusters of borrowers instead of individuals. And, that's exactly what grading does - it segments borrowers based on their credit score and other behaviors, which is we should keep the grade column and drop interest int_rate and sub_grade.\n",
        "\n",
        "Let's drop these columns from the DataFrame before moving onto to the next group of columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH7Xex7O3spn"
      },
      "outputs": [],
      "source": [
        "drop_list = ['id','member_id','funded_amnt','funded_amnt_inv',\n",
        "             'int_rate','sub_grade','emp_title','issue_d']\n",
        "loans_2007 = loans_2007.drop(drop_list,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbDtCSU73spt"
      },
      "source": [
        "Second Group Of Columns\n",
        "Let's move on to the next 19 columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eeakOal3spu"
      },
      "outputs": [],
      "source": [
        "preview[19:38]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThW7rBlg3spy"
      },
      "source": [
        "In this group,take note of the fico_range_low and fico_range_high columns. Both are in this second group of columns but because they related to some other columns, we'll talk more about them after looking at the last group of columns.\n",
        "\n",
        "We can drop the following columns:\n",
        "\n",
        "zip_code - mostly redundant with the addr_state column since only the first 3 digits of the 5 digit zip code are visible.\n",
        "out_prncp - leaks data from the future.\n",
        "out_prncp_inv - also leaks data from the future.\n",
        "total_pymnt - also leaks data from the future.\n",
        "total_pymnt_inv - also leaks data from the future.\n",
        "Let's go ahead and remove these 5 columns from the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlGG9rwj3spz"
      },
      "outputs": [],
      "source": [
        "drop_cols = [ 'zip_code','out_prncp','out_prncp_inv',\n",
        "             'total_pymnt','total_pymnt_inv']\n",
        "loans_2007 = loans_2007.drop(drop_cols, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9scoFMWF3sp2"
      },
      "source": [
        "Third Group Of Columns\n",
        "Let's analyze the last group of features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6ZcPmsU3sp2"
      },
      "outputs": [],
      "source": [
        "preview[38:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmDRIFkf3sp5"
      },
      "source": [
        "In this last group of columns, we need to drop the following, all of which leak data from the future:\n",
        "\n",
        "total_rec_prncp\n",
        "total_rec_int\n",
        "total_rec_late_fee\n",
        "recoveries\n",
        "collection_recovery_fee\n",
        "last_pymnt_d\n",
        "last_pymnt_amnt\n",
        "Let's drop our last group of columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05_1jxJo3sp6"
      },
      "outputs": [],
      "source": [
        "drop_cols = ['total_rec_prncp','total_rec_int', 'total_rec_late_fee',\n",
        "             'recoveries', 'collection_recovery_fee', 'last_pymnt_d',\n",
        "             'last_pymnt_amnt']\n",
        "\n",
        "loans_2007 = loans_2007.drop(drop_cols, axis=1)\n",
        "print (loans_2007.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWhgKsGM3sp9"
      },
      "source": [
        "## Investigating FICO Score Columns\n",
        "Now, besides the explanations provided here in the Description column,let's learn more about fico_range_low, fico_range_high, last_fico_range_low, and last_fico_range_high.\n",
        "\n",
        "FICO scores are a credit score, or a number used by banks and credit cards to represent how credit-worthy a person is. While there are a few types of credit scores used in the United States, the FICO score is the best known and most widely used.\n",
        "\n",
        "When a borrower applies for a loan, Lending Club gets the borrowers credit score from FICO - they are given a lower and upper limit of the range that the borrowers score belongs to, and they store those values as fico_range_low, fico_range_high. After that, any updates to the borrowers score are recorded as last_fico_range_low, and last_fico_range_high.\n",
        "\n",
        "A key part of any data science project is to do everything you can to understand the data. While researching this data set, I found a project done in 2014 by a group of students from Stanford University on this same dataset.\n",
        "\n",
        "In the report for the project, the group listed the current credit score (last_fico_range) among late fees and recovery fees as fields they mistakenly added to the features but state that they later learned these columns all leak information into the future.\n",
        "\n",
        "However, following this group's project, another group from Stanford worked on this same Lending Club dataset. They used the FICO score columns, dropping only last_fico_range_low, in their modeling. This second group's report described last_fico_range_high as the one of the more important features in predicting accurate results.\n",
        "\n",
        "The question we must answer is, do the FICO credit scores information into the future? Recall a column is considered leaking information when especially it won't be available at the time we use our model - in this case when we use our model on future loans.\n",
        "\n",
        "This blog examines in-depth the FICO scores for lending club loans, and notes that while looking at the trend of the FICO scores is a great predictor of whether a loan will default, that because FICO scores continue to be updated by the Lending Club after a loan is funded, a defaulting loan can lower the borrowers score, or in other words, will leak data.\n",
        "\n",
        "Therefore we can safely use fico_range_low and fico_range_high, but not last_fico_range_low, and last_fico_range_high. Lets take a look at the values in these columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adAuwm5m3sp9"
      },
      "outputs": [],
      "source": [
        "print(loans_2007['fico_range_low'].unique())\n",
        "print(loans_2007['fico_range_high'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEP6PeK83sqB"
      },
      "source": [
        "## Let's get rid of the missing values, then plot histograms to look at the ranges of the two columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbdUEXQm3sqC"
      },
      "outputs": [],
      "source": [
        "fico_columns = ['fico_range_high','fico_range_low']\n",
        "\n",
        "print(loans_2007.shape[0])\n",
        "loans_2007.dropna(subset=fico_columns,inplace=True)\n",
        "print(loans_2007.shape[0])\n",
        "\n",
        "loans_2007[fico_columns].plot.hist(alpha=0.5,bins=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuSfxTJq3sqF"
      },
      "source": [
        "## Let's now go ahead and create a column for the average of fico_range_low and fico_range_high columns and name it fico_average. \n",
        "Note that this is not the average FICO score for each borrower, but rather an average of the high and low range that we know the borrower is in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouijxKTG3sqH"
      },
      "outputs": [],
      "source": [
        "loans_2007['fico_average'] = (loans_2007['fico_range_high'] + loans_2007['fico_range_low']) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARqALwdA3sqK"
      },
      "outputs": [],
      "source": [
        "# Let's check what we just did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gji-kphP3sqM"
      },
      "outputs": [],
      "source": [
        "cols = ['fico_range_low','fico_range_high']\n",
        "loans_2007[cols].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzaylQvO3sqP"
      },
      "source": [
        "## Good! We got the mean calculations and everything right. \n",
        "Now, we can go ahead and drop fico_range_low, fico_range_high, last_fico_range_low, and last_fico_range_high columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yAq82Gu3sqP"
      },
      "outputs": [],
      "source": [
        "drop_cols = ['fico_range_low','fico_range_high','last_fico_range_low',\n",
        "             'last_fico_range_high']\n",
        "loans_2007 = loans_2007.drop(drop_cols, axis=1)\n",
        "loans_2007.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgr0-x4D3sqS"
      },
      "source": [
        "Notice just by becoming familiar with the columns in the dataset, we're able to reduce the number of columns from 56 to 33.\n",
        "\n",
        "## Decide On A Target Column\n",
        "Now, let's decide on the appropriate column to use as a target column for modeling - keep in mind the main goal is predict who will pay off a loan and who will default.\n",
        "\n",
        "We learned from the description of columns in the preview DataFrame that loan_status is the only field in the main dataset that describe a loan status, so let's use this column as the target column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIedgnLH3sqT"
      },
      "outputs": [],
      "source": [
        "preview[preview.name == 'loan_status']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCWpN5CO3sqV"
      },
      "source": [
        "Currently, this column contains text values that need to be converted to numerical values to be able use for training a model.\n",
        "\n",
        "Let's explore the different values in this column and come up with a strategy for converting the values in this column. We'll use the DataFrame method value_counts() to return the frequency of the unique values in the loan_status column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1WxBlY_3sqW"
      },
      "outputs": [],
      "source": [
        "loans_2007[\"loan_status\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS9Y65-43sqZ"
      },
      "source": [
        "The loan status has nine different possible values!\n",
        "\n",
        "Let's learn about these unique values to determine the ones that best describe the final outcome of a loan, and also the kind of classification problem we'll be dealing with.\n",
        "\n",
        "You can read about most of the different loan statuses on the Lending Club website as well as these posts on the Lend Academy and Orchard forums. I have pulled that data together in a table below so we can see the unique values, their frequency in the dataset and what each means:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EhOzBzg3sqa"
      },
      "outputs": [],
      "source": [
        "meaning = [\n",
        "    \"Loan has been fully paid off.\",\n",
        "    \"Loan for which there is no longer a reasonable expectation of further payments.\",\n",
        "    \"While the loan was paid off, the loan application today would no longer meet the credit policy and wouldn't be approved on to the marketplace.\",\n",
        "    \"While the loan was charged off, the loan application today would no longer meet the credit policy and wouldn't be approved on to the marketplace.\",\n",
        "    \"Loan is up to date on current payments.\",\n",
        "    \"The loan is past due but still in the grace period of 15 days.\",\n",
        "    \"Loan hasn't been paid in 31 to 120 days (late on the current payment).\",\n",
        "    \"Loan hasn't been paid in 16 to 30 days (late on the current payment).\",\n",
        "    \"Loan is defaulted on and no payment has been made for more than 121 days.\"]\n",
        "\n",
        "status, count = loans_2007[\"loan_status\"].value_counts().index, loans_2007[\"loan_status\"].value_counts().values\n",
        "\n",
        "loan_statuses_explanation = pd.DataFrame({'Loan Status': status,'Count': count,'Meaning': meaning})[['Loan Status','Count','Meaning']]\n",
        "loan_statuses_explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqyiZ5Yb3sqc"
      },
      "source": [
        "Remember, our goal is to build a machine learning model that can learn from past loans in trying to predict which loans will be paid off and which won't. From the above table, only the Fully Paid and Charged Off values describe the final outcome of a loan. The other values describe loans that are still on going, and even though some loans are late on payments, we can't jump the gun and classify them as Charged Off.\n",
        "\n",
        "Also, while the Default status resembles the Charged Off status, in Lending Club's eyes, loans that are charged off have essentially no chance of being repaid while default ones have a small chance. Therefore, we should use only samples where the loan_status column is 'Fully Paid' or 'Charged Off'.\n",
        "\n",
        "We're not interested in any statuses that indicate that the loan is ongoing or in progress, because predicting that something is in progress doesn't tell us anything.\n",
        "\n",
        "Since we're interested in being able to predict which of these 2 values a loan will fall under, we can treat the problem as binary classification.\n",
        "\n",
        "Let's remove all the loans that don't contain either 'Fully Paid' or 'Charged Off' as the loan's status and then transform the 'Fully Paid' values to 1 for the positive case and the 'Charged Off' values to 0 for the negative case.\n",
        "\n",
        "This will mean that out of the ~42,000 rows we have, we'll be removing just over 3,000.\n",
        "\n",
        "There are few different ways to transform all of the values in a column, we'll use the DataFrame method replace().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRVZVp-f3sqd"
      },
      "outputs": [],
      "source": [
        "loans_2007 = loans_2007[(loans_2007[\"loan_status\"] == \"Fully Paid\") |\n",
        "                            (loans_2007[\"loan_status\"] == \"Charged Off\")]\n",
        "\n",
        "mapping_dictionary = {\"loan_status\":{ \"Fully Paid\": 1, \"Charged Off\": 0}}\n",
        "loans_2007 = loans_2007.replace(mapping_dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZfHklP23sqi"
      },
      "source": [
        "## Visualizing the Target Column Outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Avb4Zj3sqk"
      },
      "outputs": [],
      "source": [
        "print(loans_2007.shape)\n",
        "loans_2007.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loans_2007['loan_status'].value_counts()"
      ],
      "metadata": {
        "id": "6e2doxfCeoxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,2,figsize=(14,7))\n",
        "sns.countplot(x='loan_status',data=loans_2007,ax=axs[0])\n",
        "axs[0].set_title(\"Frequency of each Loan Status\")\n",
        "loans_2007.loan_status.value_counts().plot(x=None,y=None, kind='pie', ax=axs[1],autopct='%1.2f%%')\n",
        "axs[1].set_title(\"Percentage of each Loan status\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d4n1AOdkdWCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0j33emm3sqm"
      },
      "source": [
        "These plots indicate that a significant number of borrowers in our dataset paid off their loan - 85.62% of loan borrowers paid off amount borrowed, while 14.38% unfortunately defaulted. From our loan data it is these 'defaulters' that we're more interested in filtering out as much as possible to reduce loses on investment returns.\n",
        "\n",
        "## Remove Columns with only One Value\n",
        "To wrap up this section, let's look for any columns that contain only one unique value and remove them. These columns won't be useful for the model since they don't add any information to each loan application. In addition, removing these columns will reduce the number of columns we'll need to explore further in the next stage.\n",
        "\n",
        "The pandas Series method nunique() returns the number of unique values, excluding any null values. We can use apply this method across the dataset to remove these columns in one easy step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK2VXnxg3sqn"
      },
      "outputs": [],
      "source": [
        "loans_2007 = loans_2007.loc[:,loans_2007.apply(pd.Series.nunique) != 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Foi84SL3sqp"
      },
      "source": [
        "Again, there may be some columns with more than one unique values but one of the values has insignificant frequency in the dataset. Let's find out and drop such column(s):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTE14C-13sqq"
      },
      "outputs": [],
      "source": [
        "print(loans_2007.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0GJRHhv3sqs"
      },
      "outputs": [],
      "source": [
        "# just preview & NOTICE column 'pymnt_plan\n",
        "for col in loans_2007.columns:\n",
        "    if (len(loans_2007[col].unique()) < 4):\n",
        "        print(loans_2007[col].value_counts())\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovATo5bj3squ"
      },
      "source": [
        "## The payment plan column (pymnt_plan) has two unique values, 'y' and 'n', with 'y' occurring only once. Let's drop this column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysgj93Zs3sqv"
      },
      "outputs": [],
      "source": [
        "print(loans_2007.shape[1])\n",
        "loans_2007 = loans_2007.drop('pymnt_plan', axis=1)\n",
        "print(\"We've been able to reduced the features to => {}\".format(loans_2007.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CFJv48r3sqx"
      },
      "source": [
        "Lastly, lets save our work in this section to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ0P9th-3sqx"
      },
      "outputs": [],
      "source": [
        "loans_2007.to_csv(\"filtered_loans_2007.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQuApuDf3sqz"
      },
      "source": [
        "# 3) Preparing the Features for Machine Learning\n",
        "In this section, we'll prepare the filtered_loans_2007.csv data for machine learning. We'll focus on handling missing values, converting categorical columns to numeric columns and removing any other extraneous columns.\n",
        "\n",
        "We need to handle missing values and categorical features before feeding the data into a machine learning algorithm, because the mathematics underlying most machine learning models assumes that the data is numerical and contains no missing values. To reinforce this requirement, scikit-learn will return an error if you try to train a model using data that contain missing values or non-numeric values when working with models like linear regression and logistic regression.\n",
        "\n",
        "Here's an outline of what we'll be doing in this stage:\n",
        "\n",
        "Handle Missing Values\n",
        "Investigate Categorical Columns\n",
        "Convert Categorical Columns To Numeric Features\n",
        "Map Ordinal Values To Integers\n",
        "Encode Nominal Values As Dummy Variables\n",
        "First though, let's load in the data from last section's final output:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzxA2SWi3sq1"
      },
      "outputs": [],
      "source": [
        "filtered_loans = pd.read_csv('filtered_loans_2007.csv')\n",
        "print(filtered_loans.shape)\n",
        "filtered_loans.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpe5sFPJ3sq3"
      },
      "source": [
        "## Handle Missing Values\n",
        "Let's compute the number of missing values and determine how to handle them. We can return the number of missing values across the DataFrame by:\n",
        "\n",
        "First, use the Pandas DataFrame method isnull() to return a DataFrame containing Boolean values:\n",
        "True if the original value is null\n",
        "False if the original value isn't null\n",
        "Then, use the Pandas DataFrame method sum() to calculate the number of null values in each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_V2EH043sq4"
      },
      "outputs": [],
      "source": [
        "null_counts = filtered_loans.isnull().sum()\n",
        "print(\"Number of null values in each column:\\n{}\".format(null_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9HjghEl3sq8"
      },
      "outputs": [],
      "source": [
        "print(filtered_loans.shape)\n",
        "filtered_loans = filtered_loans.drop(\"pub_rec_bankruptcies\",axis=1)\n",
        "print(filtered_loans.shape)\n",
        "\n",
        "filtered_loans = filtered_loans.dropna()\n",
        "print(filtered_loans.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAi57OQr3sq_"
      },
      "source": [
        "Next, we'll focus on the categorical columns.\n",
        "\n",
        "## Investigate Categorical Columns\n",
        "Keep in mind, the goal in this section is to have all the columns as numeric columns (int or float data type), and containing no missing values. We just dealt with the missing values, so let's now find out the number of columns that are of the object data type and then move on to process them into numeric form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HivWyBqX3srB"
      },
      "outputs": [],
      "source": [
        "print(\"Data types and their frequency\\n{}\".format(filtered_loans.dtypes.value_counts()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQs-N54i3srC"
      },
      "outputs": [],
      "source": [
        "object_columns_df = filtered_loans.select_dtypes(include=['object'])\n",
        "print(object_columns_df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UVRNll33srE",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#filtered_loans['revol_util'] = filtered_loans['revol_util'].str.rstrip('%').astype('float')\n",
        "cols = ['home_ownership', 'grade','verification_status', 'emp_length', 'term', 'addr_state','last_credit_pull_d']\n",
        "for name in cols:\n",
        "    print(name,':')\n",
        "    print(object_columns_df[name].value_counts(),'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tH2GNmJ3srH"
      },
      "outputs": [],
      "source": [
        "for name in ['purpose','title']:\n",
        "    print(\"Unique Values in column: {}\\n\".format(name))\n",
        "    print(filtered_loans[name].value_counts(),'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ5TQOLn3srJ"
      },
      "outputs": [],
      "source": [
        "# drop dates & large cardinality varibales\n",
        "drop_cols = ['last_credit_pull_d','addr_state','title','earliest_cr_line']\n",
        "filtered_loans = filtered_loans.drop(drop_cols,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpvvLC5w3srL"
      },
      "source": [
        "## Convert Categorical Columns to Numeric Features\n",
        "First, let's understand the two types of categorical features we have in our dataset and how we can convert each to numerical features:\n",
        "\n",
        "Ordinal values: these categorical values are in natural order. That's you can sort or order them either in increasing or decreasing order. For instance, we learnt earlier that Lending Club grade loan applicants from A to G, and assign each applicant a corresponding interest rate - grade A is less riskier while grade B is riskier than A in that order:\n",
        "A < B < C < D < E < F < G ; where < means less riskier than\n",
        "Nominal Values: these are regular categorical values. You can't order nominal values. For instance, while we can order loan applicants in the employment length column (emp_length) based on years spent in the workforce:\n",
        "year 1 < year 2 < year 3 ... < year N,\n",
        "we can't do that with the column purpose. It wouldn't make sense to say:\n",
        "\n",
        "car < wedding < education < moving < house\n",
        "These are the columns we now have in our dataset:\n",
        "\n",
        "Ordinal Values\n",
        "grade\n",
        "emp_length\n",
        "Nominal Values\n",
        "_ home_ownership\n",
        "verification_status\n",
        "purpose\n",
        "term\n",
        "There are different approaches to handle each of these two types. In the steps following, we'll convert each of them accordingly.\n",
        "\n",
        "To map the ordinal values to integers, we can use the pandas DataFrame method replace() to map both grade and emp_length to appropriate numeric values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt8ePtnG3srL"
      },
      "outputs": [],
      "source": [
        "mapping_dict = {\n",
        "    \"emp_length\": {\n",
        "        \"10+ years\": 10,\n",
        "        \"9 years\": 9,\n",
        "        \"8 years\": 8,\n",
        "        \"7 years\": 7,\n",
        "        \"6 years\": 6,\n",
        "        \"5 years\": 5,\n",
        "        \"4 years\": 4,\n",
        "        \"3 years\": 3,\n",
        "        \"2 years\": 2,\n",
        "        \"1 year\": 1,\n",
        "        \"< 1 year\": 0,\n",
        "        \"n/a\": 0\n",
        "\n",
        "    },\n",
        "    \"grade\":{\n",
        "        \"A\": 1,\n",
        "        \"B\": 2,\n",
        "        \"C\": 3,\n",
        "        \"D\": 4,\n",
        "        \"E\": 5,\n",
        "        \"F\": 6,\n",
        "        \"G\": 7\n",
        "    }\n",
        "}\n",
        "\n",
        "filtered_loans = filtered_loans.replace(mapping_dict)\n",
        "filtered_loans[['emp_length','grade']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcaVaQQX3srN"
      },
      "source": [
        "Perfect! Let's move on to the Nominal Values. The approach to converting nominal features into numerical features is to encode them as dummy variables. The process will be:\n",
        "\n",
        "Use pandas' get_dummies() method to return a new DataFrame containing a new column for each dummy variable\n",
        "Use the concat() method to add these dummy columns back to the original DataFrame\n",
        "Then drop the original columns entirely using the drop method\n",
        "Lets' go ahead and encode the nominal columns that we now have in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-pz8ODH3srO"
      },
      "outputs": [],
      "source": [
        "nominal_columns = [\"home_ownership\", \"verification_status\", \"purpose\", \"term\"]\n",
        "dummy_df = pd.get_dummies(filtered_loans[nominal_columns], drop_first=False) \n",
        "filtered_loans_with_dummy_df = pd.concat([filtered_loans, dummy_df], axis=1)\n",
        "filtered_loans_with_dummy_df = filtered_loans_with_dummy_df.drop(nominal_columns, axis=1)\n",
        "print(filtered_loans_with_dummy_df.shape)\n",
        "#(38123, 39)\n",
        "#(38123, 35) << get_dummies, drop_first=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EBh-23z3srQ"
      },
      "outputs": [],
      "source": [
        "filtered_loans_with_dummy_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eJbeF_a3srS"
      },
      "source": [
        "To wrap things up, let's inspect our final output from this section to make sure all the features are of the same length, contain no null value, and are numericals.\n",
        "\n",
        "Let's use pandas info method to inspect the filtered_loans DataFrame:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxvwXBVrW-lX"
      },
      "source": [
        "### Convert Categorical Columns to Numeric Features (OneHotEncoder)\n",
        "\n",
        "Credit: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "\n",
        "Encode categorical features as a one-hot numeric array.\n",
        "\n",
        "The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter)\n",
        "\n",
        "By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually.\n",
        "\n",
        "This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.\n",
        "\n",
        "Note: a one-hot encoding of y labels should use a LabelBinarizer instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7w6bmO1W-lX"
      },
      "outputs": [],
      "source": [
        "#------------------------------\n",
        "# Alternative solution: OneHotEncoder\n",
        "# JUST SHOW (NOW USE IT)\n",
        "#------------------------------\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# creating instance of one-hot-encoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore') # this feature will be all zeros\n",
        "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
        "nominal_columns = [\"home_ownership\", \"verification_status\", \"purpose\", \"term\"]\n",
        "enc_df = pd.DataFrame(enc.fit_transform(filtered_loans[nominal_columns]).toarray())\n",
        "enc_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEOoTX5p3srS"
      },
      "outputs": [],
      "source": [
        "filtered_loans_with_dummy_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYngCIv3srU"
      },
      "source": [
        "## Save to CSV\n",
        "It is a good practice to store the final output of each section or stage of your workflow in a separate csv file. One of the benefits of this practice is that it helps us to make changes in our data processing flow without having to recalculate everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c4svmQ_3srU"
      },
      "outputs": [],
      "source": [
        "filtered_loans_with_dummy_df.to_csv(\"cleaned_loans_2007.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qGJdrt4Rikr9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "Lab1_LoansDataSet.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}