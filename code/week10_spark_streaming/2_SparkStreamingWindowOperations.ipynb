{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCHHw7JmBE-R"
      },
      "source": [
        "# Spark Preparation\n",
        "We check if we are in Google Colab.  If this is the case, install all necessary packages.\n",
        "\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.2.1 with hadoop 3.2, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab.\n",
        "Learn more from [A Must-Read Guide on How to Work with PySpark on Google Colab for Data Scientists!](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5IklZSTX4mG"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XiZNubiX9nq"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "    !wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "    !tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "    !mv spark-3.2.1-bin-hadoop3.2 spark\n",
        "    !pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBwkaDE7O0PJ"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "  os.environ[\"SPARK_HOME\"] = \"/content/spark\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvbv_avAY5b_"
      },
      "source": [
        "# Start a Local Cluster\n",
        "Use findspark.init() to start a local cluster.  If you plan to use remote cluster, skip the findspark.init() and change the cluster_url according."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQg0Ed6cOl5b"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnv_LFtp9w4Y"
      },
      "source": [
        "For Spark Streaming, we will need **at least 2 cores** for operation, receiving data (socket, kafka, etc.) and processing data.  We will use **'local[2]'** for our local cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b715cKafZNkF"
      },
      "outputs": [],
      "source": [
        "cluster_url = 'local[2]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuvWws-GCpMg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(cluster_url)\\\n",
        "        .appName(\"Spark Streaming\")\\\n",
        "        .config('spark.ui.port', '4040')\\\n",
        "        .getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4KY8J5jBB3e"
      },
      "source": [
        "# Basic Spark Streaming Commands\n",
        "\n",
        "Create a streaming context with 5-second mini-batch interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytNE6TgaBB3j"
      },
      "outputs": [],
      "source": [
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "ssc = StreamingContext(sc, 5)\n",
        "ssc.checkpoint('./checkpoints/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsAXLOt9w4a"
      },
      "source": [
        "Due to network setup difficulties, we will use queue of RDDs as our input streams.  You can find another version of socketTextStream in the spark streaming programming guide.  However, it is just substitue the next few code blocks with:\n",
        "\n",
        "`lines = ssc.socketTextStream(\"localhost\", 9000)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c90iZClU9w4a"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/kaopanboonyuen/2110446_DataScience_2021s2/raw/main/code/week10_spark_streaming/star-wars.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYO9PGdn9w4b"
      },
      "outputs": [],
      "source": [
        "# read a text file and create a list of 10 rdds, each rdd have i lines of text\n",
        "rdds = []\n",
        "with open('star-wars.txt', encoding='ISO-8859-1') as fd:\n",
        "    for i in range(1, 10):\n",
        "        data = []\n",
        "        for k in range(i):\n",
        "            # read a line of text, strip newline at the end and also skip blank line\n",
        "            text = fd.readline().strip()\n",
        "            while not text:\n",
        "                text = fd.readline().strip()\n",
        "            data.append(text)\n",
        "        rdds.append(sc.parallelize(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bR32OzG9w4b"
      },
      "outputs": [],
      "source": [
        "lines = ssc.queueStream(rdds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo0hYwiW9w4b"
      },
      "source": [
        "## Example of word count in spark streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqWNhJT29w4c"
      },
      "outputs": [],
      "source": [
        "# Split each line into words\n",
        "words = lines.flatMap(lambda line: line.split(' '))\n",
        "\n",
        "# Count each word in each batch\n",
        "pairs = words.map(lambda word: (word, 1))\n",
        "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Window operations with varied window parameters\n",
        "twoWindowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 10, 5)\n",
        "threeWindowedTwoSlideWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 15, 10)\n",
        "\n",
        "# Print counts of a word 'the' from different calculation to the console\n",
        "wordCounts.filter(lambda x: x[0] == 'the').pprint()\n",
        "twoWindowedWordCounts.filter(lambda x: x[0] == 'the').pprint()\n",
        "threeWindowedTwoSlideWordCounts.filter(lambda x: x[0] == 'the').pprint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyM_KVcJ9w4c"
      },
      "source": [
        "## Trigger the stream processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fSUBQ5P9w4c"
      },
      "outputs": [],
      "source": [
        "ssc.start()\n",
        "\n",
        "# we will wait for 60 seconds and then continue to stop the stream processing\n",
        "# we can wait forever with empty parameter\n",
        "ssc.awaitTermination(60)\n",
        "\n",
        "# stop streaming context, this is also stop spark context\n",
        "ssc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgHlxsif9w4d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2-SparkStreamingWindowOperations.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}